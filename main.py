"""
FastAPI åç«¯ï¼šå°è£… TableMerger + DataHealthScannerï¼Œæä¾› /merge-and-scanï¼Œé»˜è®¤è¿è¡Œåœ¨ 5001 ç«¯å£ï¼ˆé¿å…ä¸ç³»ç»Ÿå ç”¨ 5000 å†²çªï¼‰ã€‚
å¯åŠ¨: uvicorn main:app --reload --host 0.0.0.0 --port 5001
"""

from __future__ import annotations

import hashlib
import json
import sys
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Tuple

ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT / "src"))

from fastapi import FastAPI, File, Form, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from core.merger import TableMerger, analyze_headers_with_strategy_from_contents
from core.scanner import scan_health

app = FastAPI(title="Merge & Health Scan API", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æºï¼Œè¿™æ · Render ä¸Šçš„å‰ç«¯æ‰èƒ½è¿ä¸Š
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ğŸ’¡ æ–°å¢ï¼šå¥åº·æ£€æŸ¥å’ŒçŠ¶æ€æŸ¥è¯¢æ¥å£ ---

@app.get("/health")
def health_check():
    """è®©å‰ç«¯çŸ¥é“åç«¯å¤§è„‘è¿˜æ´»ç€"""
    return {"status": "ok"}

@app.get("/check-status")
def check_status():
    """è®©å‰ç«¯çŸ¥é“åç«¯ç°åœ¨æ˜¯å¦ç©ºé—²"""
    return {"status": "idle"}

# --- ğŸ‘† ç²˜è´´ç»“æŸ ---
# å½“å‰å¤„ç†æ–‡ä»¶çš„æŒ‡çº¹ï¼ˆmerge-and-scan æˆåŠŸåæ›´æ–°ï¼‰ï¼Œä¾› /api/check-status å¯¹æ¯”
_last_merge_fingerprint: str | None = None

# analyze-headers ä¸ merge-and-scan ä¹‹é—´çš„å†…å­˜ç¼“å­˜ï¼Œé¿å…æ–‡ä»¶å¯¹è±¡è¢« GC æå‰é”€æ¯
_upload_cache: Dict[str, List[Tuple[str, bytes]]] = {}
_CACHE_MAX_ENTRIES = 20


def df_to_merged_json(df):
    """DataFrame è½¬å‰ç«¯ MergedDataï¼šcolumns + rowsï¼ŒNaN ä¸º nullã€‚"""
    records = json.loads(df.to_json(orient="records", date_format="iso", force_ascii=False))
    out = []
    for row in records:
        out.append({k: (None if v is None else str(v)) for k, v in row.items()})
    return {"columns": list(df.columns), "rows": out}


@app.get("/")
def root():
    return {
        "service": "Merge & Health Scan API",
        "docs": "/docs",
        "analyze_headers": "POST /api/analyze-headers",
        "merge_and_scan": "POST /api/merge-and-scan",
        "check_status": "GET /api/check-status",
    }


@app.get("/api/check-status")
def check_status():
    """
    è¿”å›å½“å‰ç³»ç»Ÿå†…å­˜ä¸­è®°å½•çš„åˆå¹¶æŒ‡çº¹ã€‚
    å‰ç«¯è½®è¯¢å¯¹æ¯”ï¼šè‹¥ä¸æœ¬åœ°ä¿å­˜çš„æŒ‡çº¹ä¸ä¸€è‡´ï¼Œåˆ™æç¤ºã€Œæºæ–‡æ¡£å·²æ›´æ–°ã€å¹¶å±•ç¤ºã€ŒåŒæ­¥æ›´æ–°ã€ã€‚
    """
    global _last_merge_fingerprint
    return {"fingerprint": _last_merge_fingerprint}


@app.get("/health")
def health():
    """å¥åº·æ£€æŸ¥ï¼Œä¾›å‰ç«¯æˆ–ä»£ç†ç¡®è®¤åç«¯å·²å¯åŠ¨ã€‚"""
    return {"status": "ok", "port": 5001}


# ä¸ DataHealthScanner é»˜è®¤è§„åˆ™ä¸€è‡´
DEFAULT_NUMERIC_COLUMNS = ["åˆ†æ•°"]
DEFAULT_COMPOSITE_KEY_COLUMNS = ["å§“å", "ç­çº§"]
# æ ¹æ®åˆ—åå…³é”®è¯æ¨æ–­ï¼šå¼‚å¸¸å€¼ç›‘æ§ã€æ—¥æœŸ/é‚®ç®±æ ¼å¼ã€æ•°å€¼ç±»å‹ï¼ˆå¦‚æ¥¼å±‚ï¼‰
OUTLIER_KEYWORDS = ["åˆ†æ•°", "é¢ç§¯", "æ•°é‡", "åº§ä½æ•°", "é‡‘é¢", "ä»·æ ¼", "æ•°å€¼"]
DATE_KEYWORDS = ["æ—¥æœŸ", "æ—¶é—´", "å¯ç”¨æ—¥æœŸ", "åˆ›å»ºæ—¥æœŸ"]
EMAIL_KEYWORDS = ["é‚®ç®±", "é‚®ä»¶", "email"]
# è¯­ä¹‰ä¸Šåº”ä¸ºæ•°å€¼çš„åˆ—ï¼ˆç±»å‹æ ¡éªŒï¼šéæ•°å­—å¦‚ã€Œä¸€æ¥¼ã€ä¼šæ ‡ä¸ºç±»å‹ä¸ä¸€è‡´ï¼‰
NUMERIC_TYPE_KEYWORDS = ["æ¥¼å±‚", "å±‚", "æ¥¼", "ç¼–å·", "åºå·"]
# æ˜ç¡®ä¸ºæ–‡æœ¬/åç§°åˆ—ï¼šä¸åšæ•°å€¼ã€æ—¥æœŸã€å¼‚å¸¸å€¼è§„åˆ™ï¼Œé¿å…è¯¯æ ‡ï¼ˆå¦‚ã€Œå­¦æ ¡åç§°ã€ï¼‰
TEXT_NAME_KEYWORDS = ["åç§°", "åå­—", "æ ‡é¢˜", "è¯´æ˜", "å¤‡æ³¨", "æè¿°", "å­¦æ ¡åç§°"]
# æ¯”é‡/æ¯”ä¾‹åˆ—ï¼šåªåšæ•°å€¼ç±»å‹æ ¡éªŒï¼Œä¸åš IQR å¼‚å¸¸å€¼ï¼ˆå¦‚ã€Œå åŠå…¬ä¸æ•™å®¤çš„å®¤å†…é¢ç§¯æ¯”é‡ã€ï¼‰
PROPORTION_KEYWORDS = ["æ¯”é‡", "æ¯”ä¾‹", "ç‡"]


def _is_text_name_column(col_name: str) -> bool:
    """åˆ¤å®šæ˜¯å¦ä¸ºæ–‡æœ¬/åç§°ç±»åˆ—ï¼Œæ­¤ç±»åˆ—ä¸æ–½åŠ æ•°å€¼ã€æ—¥æœŸã€å¼‚å¸¸å€¼è§„åˆ™ã€‚"""
    c = col_name.strip()
    return any(kw in c for kw in TEXT_NAME_KEYWORDS)


def _is_proportion_column(col_name: str) -> bool:
    """åˆ¤å®šæ˜¯å¦ä¸ºæ¯”é‡/æ¯”ä¾‹ç±»åˆ—ï¼Œæ­¤ç±»åˆ—åªåšç±»å‹æ ¡éªŒï¼Œä¸åš IQR å¼‚å¸¸å€¼ã€‚"""
    c = col_name.strip()
    return any(kw in c for kw in PROPORTION_KEYWORDS)


def _infer_rules_from_columns(base_columns: list) -> dict:
    """
    æ ¹æ®è¡¨å¤´åç§°æ¨æ–­ä¸“ä¸šæ¸…æ´—è§„åˆ™ï¼Œä¾› propose-rules ä¸ merge-and-scan å…±ç”¨ã€‚
    åŠ å¼ºå­—æ®µç±»å‹åˆ†æï¼šæ–‡æœ¬/åç§°åˆ—ä¸æ–½åŠ æ•°å€¼/æ—¥æœŸ/å¼‚å¸¸å€¼ï¼›æ¯”é‡åˆ—ä¸åš IQR å¼‚å¸¸å€¼ã€‚
    è¿”å›ï¼šrequired_columns, numeric_columns, composite_key_columns,
          outlier_columns, pattern_columns, constraints, proposed_rules(å‰ç«¯å±•ç¤ºç”¨)ã€‚
    """
    base_set = set(base_columns)
    required_columns = list(base_columns)
    numeric_columns = [c for c in DEFAULT_NUMERIC_COLUMNS if c in base_set]
    composite_key_columns = [c for c in DEFAULT_COMPOSITE_KEY_COLUMNS if c in base_set]
    outlier_columns = []
    pattern_columns = {}
    proposed_rules = []

    for col in base_columns:
        col_lower = col.lower().strip()
        col_any = col.strip()
        if _is_text_name_column(col):
            continue
        if any(kw in col_any for kw in OUTLIER_KEYWORDS):
            if col not in numeric_columns:
                numeric_columns.append(col)
            if not _is_proportion_column(col) and col not in outlier_columns:
                outlier_columns.append(col)
        if any(kw in col_any for kw in NUMERIC_TYPE_KEYWORDS):
            if col not in numeric_columns:
                numeric_columns.append(col)
        if any(kw in col_any for kw in DATE_KEYWORDS):
            pattern_columns[col] = r"^\d{4}[-/]\d{1,2}[-/]\d{1,2}$"
            proposed_rules.append({
                "rule_type": "pattern",
                "columns": [col],
                "description": f"ã€Œ{col}ã€æ—¥æœŸæ ¼å¼ç»Ÿä¸€",
                "severity": "business",
                "handling": "ç»Ÿä¸€ä¸º YYYY-MM-DD æˆ– YYYY/MM/DDï¼Œéæ—¥æœŸï¼ˆå¦‚å¾…å®šã€æ— è®°å½•ï¼‰å°†æ ‡å‡º",
            })
        if any(kw in col_any or kw in col_lower for kw in EMAIL_KEYWORDS):
            pattern_columns[col] = r"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$"
            proposed_rules.append({
                "rule_type": "pattern",
                "columns": [col],
                "description": f"ã€Œ{col}ã€é‚®ç®±æ ¼å¼æ ¡éªŒ",
                "severity": "business",
                "handling": "ç¬¦åˆé‚®ç®±æ ¼å¼",
            })

    for c in outlier_columns:
        proposed_rules.append({
            "rule_type": "outlier",
            "columns": [c],
            "description": f"ã€Œ{c}ã€å¼‚å¸¸å€¼ç›‘æ§",
            "severity": "business",
            "handling": "ç¡®è®¤æˆ–ä¿®æ­£è¶…å‡ºæ­£å¸¸èŒƒå›´çš„æ•°å€¼",
        })

    return {
        "required_columns": required_columns,
        "numeric_columns": numeric_columns,
        "composite_key_columns": composite_key_columns,
        "outlier_columns": outlier_columns,
        "pattern_columns": pattern_columns,
        "constraints": [],
        "proposed_rules": proposed_rules,
    }


@app.post("/get-scan-rules")
async def get_scan_rules(body: dict):
    """
    æ ¹æ®å¯¹é½åçš„è¡¨å¤´åŠ¨æ€è¿”å› DataHealthScanner å°†è¦æ‰§è¡Œçš„è§„åˆ™åˆ—è¡¨ã€‚
    ä¾›å‰ç«¯ã€Œè§„åˆ™ç¡®è®¤ã€çœ‹æ¿é€æ˜åŒ–å±•ç¤ºï¼šç©ºå€¼è§„åˆ™ã€ç±»å‹è§„åˆ™ã€å»é‡è§„åˆ™ã€‚
    """
    base_columns = body.get("base_columns")
    if not isinstance(base_columns, list):
        base_columns = []
    out = _infer_rules_from_columns(base_columns)
    return {
        "required_columns": out["required_columns"],
        "numeric_columns": out["numeric_columns"],
        "composite_key_columns": out["composite_key_columns"],
    }


@app.post("/propose-rules")
async def propose_rules(body: dict):
    """
    AI è‡ªåŠ¨æ¨æ–­ï¼šæ ¹æ®è¡¨å¤´åç§°ï¼ˆæ—¥æœŸã€é¢ç§¯ã€é‚®ç®±ç­‰ï¼‰åŒ¹é…ä¸“ä¸šæ¸…æ´—è§„åˆ™ã€‚
    è¿”å›ï¼šbasic åŸºç¡€è§„åˆ™ + proposed ä¸“ä¸šè§„åˆ™åˆ—è¡¨ï¼ˆè§„åˆ™æè¿°ã€ä¸¥é‡ç¨‹åº¦ã€é¢„æœŸå¤„ç†æ–¹å¼ï¼‰ã€‚
    """
    base_columns = body.get("base_columns")
    if not isinstance(base_columns, list):
        base_columns = []
    out = _infer_rules_from_columns(base_columns)
    return {
        "basic": {
            "required_columns": out["required_columns"],
            "numeric_columns": out["numeric_columns"],
            "composite_key_columns": out["composite_key_columns"],
        },
        "proposed": out["proposed_rules"],
    }


def _evict_cache_if_needed() -> None:
    """ç¼“å­˜æ¡ç›®è¿‡å¤šæ—¶åˆ é™¤æœ€æ—§çš„ä¸€æ¡ï¼ˆFIFO ç®€å•å®ç°ï¼‰ã€‚"""
    global _upload_cache
    if len(_upload_cache) < _CACHE_MAX_ENTRIES:
        return
    key_to_remove = next(iter(_upload_cache), None)
    if key_to_remove is not None:
        del _upload_cache[key_to_remove]


@app.post("/analyze-headers")
async def api_analyze_headers(files: list[UploadFile] = File(..., description="å¤šä¸ª CSV æ–‡ä»¶")):
    """
    è½»é‡çº§æ¥å£ï¼šä»å†…å­˜è§£æè¡¨å¤´ï¼Œå¹¶å°†æ–‡ä»¶å†…å®¹å†™å…¥å†…å­˜ç¼“å­˜ï¼Œä¾›åç»­ merge-and-scan ä½¿ç”¨ï¼Œ
    é¿å…æ–‡ä»¶å¯¹è±¡è¢« Python GC æå‰é”€æ¯ï¼ˆä¿®å¤ FileNotFoundï¼‰ã€‚
    è¿”å›æ ¼å¼é€‚é…å‰ç«¯ HeaderPreviewï¼šbase_columnsã€filesã€previewã€cache_keyã€‚
    """
    csv_files = [f for f in files if f.filename and f.filename.lower().endswith(".csv")]
    if not csv_files:
        raise HTTPException(400, "è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ª CSV æ–‡ä»¶")

    file_entries: list[tuple[str, bytes]] = []
    for f in csv_files:
        content = await f.read()
        name = f.filename or "upload.csv"
        file_entries.append((name, content))
    file_entries.sort(key=lambda x: x[0])

    _evict_cache_if_needed()
    cache_key = uuid.uuid4().hex
    _upload_cache[cache_key] = file_entries

    result = analyze_headers_with_strategy_from_contents(file_entries)
    result["cache_key"] = cache_key
    return result


@app.post("/merge-and-scan")
async def merge_and_scan(
    files: list[UploadFile] = File(None, description="å¤šä¸ª CSV æ–‡ä»¶ï¼›è‹¥ä¼  cache_key å¯çœç•¥"),
    merge_strategy: str = Form("template", description="intersection | union | template"),
    baseline_columns: str = Form(None, description="JSON æ•°ç»„ï¼Œç­–ç•¥ä¸º intersection/union æ—¶å¿…ä¼ "),
    primary_key_columns: str = Form(None, description="JSON æ•°ç»„ï¼Œä¸»é”®åˆ—ç”¨äºå»é‡"),
    template_incremental: str = Form("false", description="æŒ‰æ¨¡æ¿æ—¶æ˜¯å¦å°†å¤šä½™åˆ—ä½œä¸ºå¢é‡åˆå¹¶ï¼štrue | false"),
    cache_key: str = Form(None, description="analyze-headers è¿”å›çš„ç¼“å­˜é”®ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜é¿å… GC é”€æ¯"),
):
    """æ¥æ”¶å¤šä¸ª CSVï¼ˆæˆ– cache_keyï¼‰åŠå¯é€‰ç­–ç•¥/ä¸»é”®ï¼Œè°ƒç”¨ TableMerger.merge_and_report + DataHealthScanner.scanã€‚"""
    file_entries: list[tuple[str, bytes]] | None = None
    if cache_key and cache_key.strip() and cache_key in _upload_cache:
        file_entries = _upload_cache.pop(cache_key, None)

    if file_entries is None or not file_entries:
        csv_files = [f for f in (files or []) if f and f.filename and f.filename.lower().endswith(".csv")]
        if not csv_files:
            raise HTTPException(400, "è¯·ä¸Šä¼  CSV æ–‡ä»¶æˆ–æä¾›æœ‰æ•ˆçš„ cache_keyï¼ˆå…ˆè°ƒç”¨ analyze-headersï¼‰")

    baseline_list = None
    if baseline_columns:
        try:
            parsed = json.loads(baseline_columns)
            if isinstance(parsed, list) and len(parsed) > 0:
                baseline_list = parsed
        except (json.JSONDecodeError, TypeError):
            pass

    primary_key_list = None
    if primary_key_columns:
        try:
            primary_key_list = json.loads(primary_key_columns)
            if not isinstance(primary_key_list, list):
                primary_key_list = None
        except (json.JSONDecodeError, TypeError):
            primary_key_list = None

    if file_entries is not None:
        file_entries_sorted = sorted(file_entries, key=lambda x: x[0])
        file_contents = [c for _, c in file_entries_sorted]
    else:
        file_entries_sorted = []
        file_contents = []
        for f in csv_files:
            content = await f.read()
            name = f.filename or "upload.csv"
            file_entries_sorted.append((name, content))
        file_entries_sorted.sort(key=lambda x: x[0])
        file_contents = [c for _, c in file_entries_sorted]

    with tempfile.TemporaryDirectory(prefix="merge_scan_") as tmpdir:
        paths = []
        for name, content in file_entries_sorted:
            path = Path(tmpdir) / name
            path.write_bytes(content)
            paths.append(str(path))
        paths.sort(key=lambda p: Path(p).name)
        h = hashlib.sha256()
        for content in file_contents:
            h.update(content)
        fingerprint = h.hexdigest()
        incremental = template_incremental.strip().lower() in ("true", "1", "yes")
        merger = TableMerger()
        df, schema_report = merger.merge_and_report(
            paths,
            baseline_columns=baseline_list,
            template_incremental=incremental,
            primary_key_columns=primary_key_list,
        )

        if "error" in schema_report and schema_report.get("merged_row_count", 0) == 0:
            return {
                "schema_report": schema_report,
                "health_manifest": {
                    "errors": [],
                    "summary": "åˆå¹¶æœªäº§ç”Ÿæ•°æ®",
                    "counts": {"structural_nulls": 0, "business_nulls": 0, "type_errors": 0, "duplicates": 0, "outliers": 0, "pattern_mismatch": 0, "constraint_violation": 0, "total": 0},
                },
                "merged": {"columns": [], "rows": []},
                "fingerprint": None,
            }

        ref_cols = schema_report.get("reference_columns") or list(df.columns)
        inferred = _infer_rules_from_columns(ref_cols)
        composite_key = primary_key_list if primary_key_list else inferred["composite_key_columns"]
        health_manifest = scan_health(
            df,
            schema_report,
            composite_key_columns=composite_key,
            numeric_columns=inferred["numeric_columns"],
            outlier_columns=inferred["outlier_columns"],
            pattern_columns=inferred["pattern_columns"],
            constraints=inferred["constraints"],
        )
        merged = df_to_merged_json(df)

        global _last_merge_fingerprint
        _last_merge_fingerprint = fingerprint

        return {
            "schema_report": schema_report,
            "health_manifest": health_manifest,
            "merged": merged,
            "fingerprint": fingerprint,
        }


if __name__ == "__main__":
    import uvicorn
    import errno

    port = 5001
    try:
        uvicorn.run(app, host="0.0.0.0", port=port)
    except OSError as e:
        if e.errno == errno.EADDRINUSE:
            print(f"\nç«¯å£ {port} å·²è¢«å ç”¨ã€‚å¯å…ˆæ€æ‰æ—§è¿›ç¨‹ï¼š")
            print(f"  macOS/Linux: lsof -ti:{port} | xargs kill -9")
            print(f"  Windows:    netstat -ano | findstr :{port} å¾—åˆ° PID åï¼Œtaskkill /PID <PID> /F\n")
        raise
